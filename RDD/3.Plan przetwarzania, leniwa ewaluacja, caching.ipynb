{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan przetwarzania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zadanie:** obliczenie sumy kwadradów :  \n",
    "\n",
    "$$\\sum_{i=1}^n x_i^2 $$ \n",
    "\n",
    "Standardowa (**rzemieślnicza**) droga\n",
    "1. Oblicz kwadrat każdego elementu.\n",
    "2. Sumuj wartości kwadratów .\n",
    "\n",
    "To podejście wymaga **przechowywania** wszystkich rezultatach pośrednich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"Figures/LazyEvaluation/Slide1.png\" style=\"height:455px;width:900px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/LazyEvaluation/Slide2.png\" style=\"height:455px; width:900px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/LazyEvaluation/Slide3.png\" style=\"height:455px; width:900px\" /></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation:\n",
    " * odkłada przetwarzanie kwadratów dopóki rezultat nie jest potrzebny;\n",
    " * nie ma potrzeby przetrzymywania rezultatów pośrednich;\n",
    " * Przeskanowuje dane raz zamiast dwóch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/LazyEvaluation/Slide4.png\" style=\"height:455px; width:900px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/LazyEvaluation/Slide5.png\" style=\"height:455px; width:900px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* w przeciwieństwie do Pythona, komendy map/reduce nie zawsze przetwarzają obliczenia w momencie wykonania. \n",
    "* Zamiast tego budują tzw **execution plan**\n",
    "* Tylko kiedy rezultat obliczeń jest potrzebny obliczenia zostają wykonane. \n",
    "* Podejście to nazywa się **lazy execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaletą lazy execution jest minimalizacja liczby dostępów do pamięci. Przykładowo : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A=RDD.map(lambda x:x*x).filter(lambda x: x%2==0)\n",
    "A.reduce(lambda x,y:x+y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polecenia definiują określony plan. Dla każdej liczby x w RDD : \n",
    "\n",
    "1. Oblicz kwadrat x\n",
    "2. Filtruj x*x którego wartość jest nieparzysta\n",
    "3. Sumuj elementy które nie zostały przefiltrowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution plan** polega na obliczeniu kwadratów wszystkich elementów w RDD, zapisywaniu wyników w nowym RDD, następnie wykonywaniu przejścia filtrującego, generowaniu drugiego RDD, a następnie wykonywaniu określonego rezultatu. Wykonanie tego będzie wymagało trzykrotnej iteracji przez RDD i utworzenia 2 tymczasowych RDD. Ponieważ dostęp do pamięci jest wąskim gardłem w tego rodzaju obliczeniach, plan wykonania jest powolny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lepszy **execution plan** jest wykonanie kolejno wszystkich trzech operacji na każdym elemencie RDD, a następnie przejście do następnego elementu. Ten plan jest szybszy, ponieważ dokonujemy iteracji elementów RDD tylko raz i dlatego, że nie musimy zapisywać wyników pośrednich. Musimy utrzymywać tylko jedną zmienną: sumę cząstkową, a ponieważ jest to pojedyncza zmienna, możemy użyć rejestru CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = \"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://deeplearning-1.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 ms, sys: 476 µs, total: 2.27 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "RDD = sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[1] at RDD at PythonRDD.scala:53 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []\n"
     ]
    }
   ],
   "source": [
    "#print execution plan\n",
    "print(RDD.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "def taketime(i):\n",
    "    [cos(j) for j in range(100)]\n",
    "    return cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58 µs, sys: 15 µs, total: 73 µs\n",
      "Wall time: 88.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5623790762907029"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taketime(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jednostki czasu\n",
    "* 1 second = 1000 Milli-second ($ms$)\n",
    "* 1 Millisecond = 1000 Micro-second ($\\mu s$)\n",
    "* 1 Microsecond = 1000 Nano-second ($ns$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clock Rate\n",
    "Jeden cykl 3GHz cpu zabiera $\\frac{1}{3} ns$\n",
    "\n",
    "`taketime(1000)` zabiera około 25 $\\mu s$ = 75,000 cykli zegarowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `map` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 µs, sys: 10 µs, total: 51 µs\n",
      "Wall time: 72.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Interm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dlaczego tak szybko ? :)\n",
    "\n",
    "* Oczekujemy że operacja map zabierze 1,000,000 * 25 µs = 25 Sekund\n",
    "* Dlaczego poprzednie polecenie zabrało 16 us ? \n",
    "* Ponieważ nie zostało dokonane żadne przetwarzanie \n",
    "* Potrzebne polecenie zdefiniowało **execution plan** lecz nie przetwarza jeszcze niczego "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________(4) odnosi się do liczby partycji "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktualny execution plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/Slide1.jpg\" style=\"height:120px; width:900px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 6.9 ms, sys: 0 ns, total: 6.9 ms\n",
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dlaczego tak szybko ? Part 2 \n",
    "* Oczekujemy że operacja mapowania zabierze 1,000,000 * 25 us = 25 sekund\n",
    "* Map+reduce zabiera tylko około ~4 seconds\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie różnych obliczeń bazujących na tym samym planie przetwarzania.¶\n",
    "\n",
    "* Plan przetwarzania zdefiniowany przez Interm może być przetwarzany więcej niż raz,\n",
    "\n",
    "* Przykład : oblicz wszystkie wyjścia funkcji map które są większe niż zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 15 ms, sys: 383 µs, total: 15.3 ms\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cena za brak materializacji\n",
    "* Czas przetwarzania (3.04 sec) jest podobny do tego przy reduce (3.77 sec).\n",
    "* Dzieje się tak ponieważ rezultat Interm nie został zapisany w pamięci(zmaterializowany)\n",
    "* W związku z czym funkcja map musi być przetworzona jeszcze raz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Środkowy blok: `Map(Taketime)` jest przetwarzany dwukrotnie.\n",
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/Slide2.jpg\" style=\"height:200px; width:900px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podstawowa idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/basic.jpg\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/basic_idea.png\" style=\"height:500px; /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/cache_hit.png\" style=\"height:500px; /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/cache_miss.png\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/cms1.png\"/></p>\n",
    "\n",
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/cms2.png\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Przetwarzanie jest **efektywne jeżeli większość dostępów to cache hit**\n",
    "* W momencie w którym następuje cache miss obliczenia zajmują o wiele więcej czasu przez latencje spowodowaną operacją odczytu i zapisu danych do pamięci cache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache'owanie rezultatów pośrednich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W wyzej wymienionym planie przetwarzania chcemy zachować pośrednie rezultaty Interm w celu ponownego użycia go bez wykonywania tych samych obliczeń n-razy\n",
    "* Metoda `cache()` wkazuje na to że wygenerowane RDD w tym planie przetwarzania powinno być przechowane w pamięci. Pamiętaj jednak że jest to **plan cache'owania**. Faktyczne cache'owanie będzie wykonywane w mommencie którym rezultat będzie potrzebny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.69 ms, sys: 805 µs, total: 8.49 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprzez dodanie cache po `Map(Taketime)`, zapisujemy rezultat mapowania dla kolejnych obliczeń\n",
    "<p><img alt=\"\" src=\"Figures/ExecutionPlan/Slide3.jpg\" style=\"height:200px; width:900px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan cache'owania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[6] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie planów z i bez cache'u\n",
    "\n",
    "**Plan z Cache**\n",
    "```\n",
    "(4) PythonRDD[5] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]\n",
    " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 [Memory Serialized 1x Replicated]\n",
    "```  \n",
    "**Plan bez Cache**\n",
    "```\n",
    "(4) PythonRDD[2] at RDD at PythonRDD.scala:48 []\n",
    " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 []\n",
    "```\n",
    "Różnica polega na tym że plan dla obu RDD zawiera **[Memory Serialized 1x Replicated]** co jest planem materializacji RDD podczas przetwarzania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie cache\n",
    "Poniższa komenda przetwarza pierwszy map-reduce cache'uje rezultat mapowania w pamięci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 6.31 ms, sys: 484 µs, total: 6.8 ms\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem Interm jest buforowany. Dlatego drugie użycie Interm jest znacznie szybsze niż wtedy, gdy nie używaliśmy cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 9.03 ms, sys: 1.08 ms, total: 10.1 ms\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie \n",
    "* Spark używa **leniwej ewaluacji** w celu zaoszczędzenia czasu i pamięci.\n",
    "* Kiedy to samo RDD jest potrzebne jako wejście dla kilku operacji przetwarzania, dobrą praktyką jest **zachować  RDD w pamięci cache** w celu zwiększenia prędkości operacji odczytu i zapisu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
